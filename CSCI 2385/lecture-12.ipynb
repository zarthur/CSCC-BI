{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey of Supervised Learning Methods\n",
    "\n",
    "In this survey of learning methods, we'll attempt to develop a classification function from training data using the following algorithms\n",
    "\n",
    "- Logistic Regression\n",
    "- Linear Discriminant Analysis\n",
    "- K-Nearest Neighbors\n",
    "- Decision Trees\n",
    "- Gaussian Naive Bayes\n",
    "- Support Vector Machines\n",
    "\n",
    "While the details of each of these algorithms is beyond the scope of this notebook, we will evaluate the fitness of the results produced by each algorithm using test data.\n",
    "\n",
    "The content of this notebook has been adapted from [*Your First Machine Learning Projet in Python Step-By-Step*](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/) by [Jason Brownlee](https://machinelearningmastery.com/author/jasonb/).\n",
    "\n",
    "## Preparing the Environment\n",
    "\n",
    "To perform these machine learning tasks, we'll make use of the following libraries and their dependencies.\n",
    "\n",
    "- sklearn - a machine learning library\n",
    "- seaborn - a data visualization library\n",
    "- pandas - library providing the data structures in which we'll store our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sklearn seaborn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also configure plotting.  First we ensure that generated plots appear in the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the figure size for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "For this lab, we'll work with the [Iris Flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) - a collection of measurements of petal and sepal length and width for three Iris species.  Our objective will be to create a model that accurately classifies iris species by petal and sepal measurements.\n",
    "\n",
    "### Loading the Data\n",
    "\n",
    "We start by loading the data.  This data set is among the example datasets included with Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = sns.load_dataset('iris')\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set includes the values noted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 150 rows of data; each of the three species has a 50 sets of measurements.  \n",
    "\n",
    "### Explore the Data\n",
    "\n",
    "Before applying the machine learning algorithms, its helpful to explore the data in order to get an understanding of the input values.  An understanding of the data will help to evaluate the models produced by the algorithms and determine how well they perform.\n",
    "\n",
    "We can start by looking at the pair-wise scatter plots of features with coloring by species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris_data, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clear divisions among the species in some of these plots.  We can also look at the distribution of data for each species using boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = sns.mpl.pyplot.subplot(2,2,1)\n",
    "sns.boxplot(x=\"species\", y=\"petal_length\", data=iris_data, ax=axes)\n",
    "axes = sns.mpl.pyplot.subplot(2,2,2)\n",
    "sns.boxplot(x=\"species\", y=\"petal_width\", data=iris_data, ax=axes)\n",
    "axes = sns.mpl.pyplot.subplot(2,2,3)\n",
    "sns.boxplot(x=\"species\", y=\"sepal_length\", data=iris_data, ax=axes)\n",
    "axes = sns.mpl.pyplot.subplot(2,2,4)\n",
    "sns.boxplot(x=\"species\", y=\"sepal_width\", data=iris_data, ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the box plots, it looks like we can determine whether or not an Iris is of the Setosa species based on the petal length or width (there is no overlap between petal measurements for this species with measurements for the others).  It it less clear how to differentiate betwee the Versicolor and Virginica species.\n",
    "\n",
    "### Separating Training and Testing Data\n",
    "\n",
    "As a next step, we can separate our dataset into two parts, one used to train a model and the other used to test the effectiveness of the trained models.  To do this, we'll make use of the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "iris_train, iris_test = train_test_split(iris_data, test_size=0.2)\n",
    "\n",
    "print(\"Training rows\")\n",
    "print(iris_train.species.value_counts())\n",
    "print(\"Testing rows\")\n",
    "print(iris_test.species.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see data from each species appears in both the training and testing data.\n",
    "\n",
    "### Evaluate Algorithms\n",
    "\n",
    "To set up the code need to run the algorithms and evaluate the resulting models fitness, we'll import submodules, classes, and function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process, we'll use a for-loop to use an algorithm with the training data an evaluate the model with the testing data.  To begin, we'll construct a dictionary that maps algorithm names with the Python objects for each algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = {}\n",
    "algorithms['LR'] = LogisticRegression()\n",
    "algorithms['NB'] = GaussianNB()\n",
    "algorithms['LDA'] = LinearDiscriminantAnalysis()\n",
    "algorithms['KNN'] = KNeighborsClassifier()\n",
    "algorithms['DTC'] = DecisionTreeClassifier()\n",
    "algorithms['SVM'] = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a dictionary to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to separate our training data into independent or input variables and dependent or output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "x_train = iris_train[['sepal_length', 'sepal_width', \n",
    "                      'petal_length', 'petal_width']]\n",
    "# output variables as an array\n",
    "y_train = iris_train[['species']].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll iterate through the collection of algorithms and calculate build the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "for name, model in algorithms.items():\n",
    "    # split data in to training at test sets\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results[name] = cv_results\n",
    "    msg = \"Name: {0}, Results Mean: {1}, Results Std: {2}\".format(name, \n",
    "                                                                  round(cv_results.mean(), 3), \n",
    "                                                                  round(cv_results.std(), 3))\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results will very depending on the selection of training data and the seed value specified before evaluating models.\n",
    "\n",
    "Let's look at more detailed results for one algorithm, K-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KNeighborsClassifier object\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# fit the training data\n",
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can use the testing data to make predictions and just the efficacy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = iris_test[['sepal_length', 'sepal_width', \n",
    "                    'petal_length', 'petal_width']]\n",
    "y_test = iris_test[['species']].values.ravel()\n",
    "\n",
    "# make predictions\n",
    "predictions = knn.predict(x_test)\n",
    "\n",
    "# display predictions and actual values\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the accuracy of the predictions against the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) in which the rows correspond to the number of elements in each category represented in the collection of actual values and rows represent the number of elements of each category in the predicted values - ideally there should only be values along the main diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit a copy of the notebook (as an ipynb, html, or PDF file) on BlackBoard.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "As the last part of the lab, we looked at the K-nearest neighbor algorithm more closely.  Chose one of the other algorithms to create a model from the Iris data, fit the data, calculate the predicted values, and print the accuracy and confusion matrix for the actual and predicted values.  \n",
    "\n",
    "**Submit a copy of the notebook (as an ipynb, html, or PDF file) on BlackBoard.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
